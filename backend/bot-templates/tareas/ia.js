import { fetch } from "undici";

// Esta funci√≥n inicializa el historial con el prompt del sistema
function inicializarHistorial(botName) {
  return [
    {
      role: "user",
      parts: [
        {
          text: `A partir de ahora sos ${botName}, un asistente digital amigable creado por TarD√≠a Cloud Bot Platform. Tu prop√≥sito es ayudar a las personas con informaci√≥n, consejos, conversaci√≥n inteligente y m√°s. Respond√© siempre con amabilidad, claridad y un toque de humor cuando sea apropiado. Manten√© las respuestas concisas pero √∫tiles.`,
        },
      ],
    },
    {
      role: "model",
      parts: [
        {
          text: `¬°Entendido! Soy ${botName} ü§ñ, tu asistente digital creado con TarD√≠a Cloud Bot Platform. Estoy aqu√≠ para ayudarte con lo que necesites.`,
        },
      ],
    },
  ];
}

// üß† Integraci√≥n con IA (Gemini) con memoria
export async function responderConIA(chatId, bot, pregunta, historialesIA) {
  const apiKey = process.env.GEMINI_API_KEY;

  if (!apiKey) {
    bot.sendMessage(chatId, "‚ùå Servicio de IA no disponible.");
    return;
  }

  const botName = process.env.BOT_NAME || "TarD√≠a";

  // 1. Obtener o inicializar el historial de esta conversaci√≥n
  let historial = historialesIA.get(chatId);
  if (!historial) {
    historial = inicializarHistorial(botName);
    historialesIA.set(chatId, historial);
  }

  // 2. A√±adir la nueva pregunta del usuario al historial
  historial.push({
    role: "user",
    parts: [{ text: pregunta }],
  });

  // 3. (Opcional pero recomendado) Limitar el tama√±o del historial para no exceder l√≠mites
  // Mantenemos el prompt del sistema y los √∫ltimos 10 mensajes (5 idas y vueltas)
  const MAX_HISTORY = 12; // 2 de prompt + 10 de conversaci√≥n
  if (historial.length > MAX_HISTORY) {
    // Quitamos los mensajes m√°s antiguos de la conversaci√≥n, pero no el prompt inicial
    historial.splice(2, historial.length - MAX_HISTORY);
  }

  const body = {
    contents: historial, // Usamos el historial completo en la petici√≥n
    generationConfig: {
      temperature: 0.7,
      topK: 40,
      topP: 0.95,
      maxOutputTokens: 1024,
    },
  };

  try {
    const res = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${apiKey}`,
      {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify(body),
      },
    );

    if (!res.ok) throw new Error(`HTTP ${res.status}`);

    const data = await res.json();
    const respuesta = data.candidates?.[0]?.content?.parts?.[0]?.text;

    if (respuesta) {
      // 4. A√±adir la respuesta del modelo al historial para la pr√≥xima vez
      historial.push({
        role: "model",
        parts: [{ text: respuesta }],
      });
      historialesIA.set(chatId, historial); // Actualizamos el historial en el mapa

      await bot.sendMessage(chatId, respuesta);
      console.log(`‚úÖ Respuesta IA enviada a chat ${chatId}`);
    } else {
      bot.sendMessage(chatId, "‚ùå No pude generar una respuesta en este momento.");
      // Quitamos la √∫ltima pregunta del usuario si el modelo no respondi√≥, para que pueda reintentar
      historial.pop();
    }
  } catch (err) {
    console.error("‚ùå Error IA:", err);
    bot.sendMessage(chatId, "‚ùå Error al generar respuesta. Int√©ntalo de nuevo.");
    // Quitamos la √∫ltima pregunta del usuario si hubo un error en la llamada
    historial.pop();
  }
}

export const iaConfig = {
  name: "ia",
  displayName: "Chat IA",
  description: "Conversaci√≥n inteligente con IA",
  icon: "fas fa-brain",
  commands: [],
  schedulable: false,
  requiresApiKey: true,
  apiKeyName: "GEMINI_API_KEY",
  isConversational: true,
};
